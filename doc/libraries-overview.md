# [Библиотеки для синтаксического анализа текста](https://nlpub.mipt.ru/%D0%9E%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0_%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%B0)

## [Natural Language Toolkit (NLTK, Python)](http://www.nltk.org/)
NLTK - это ведущая платформа для создания программ на Python для работы с данными на человеческом языке. Он предоставляет простые в использовании интерфейсы для более чем 50 корпусных и лексических ресурсов, таких как WordNet, наряду с набором библиотек обработки текста для классификации, токенизации, обработки по меткам, разметки, анализа и семантического мышления, оболочек для промышленных библиотек NLP, и активный форум для обсуждений.

Благодаря практическому руководству, в котором представлены основы программирования, а также темы компьютерной лингвистики и исчерпывающей документации по API, NLTK подходит для лингвистов, инженеров, студентов, преподавателей, исследователей и пользователей отрасли. NLTK доступен для Windows, Mac OS X и Linux. Лучше всего то, что NLTK - это бесплатный проект с открытым исходным кодом, управляемый сообществом.

NLTK был назван «прекрасным инструментом для обучения и работы в области компьютерной лингвистики с использованием Python» и «удивительной библиотекой для игры с естественным языком».

Обработка естественного языка с помощью Python представляет собой практическое введение в программирование для языковой обработки. Написанный создателями NLTK, он проводит читателя через основы написания программ на Python, работы с корпусами, категоризации текста, анализа языковой структуры и многого другого. Онлайн-версия книги была обновлена ​​для Python 3 и NLTK 3.

Некоторые простые вещи, которые вы можете сделать с NLTK
Токенизировать и пометить текст:


`>>> import nltk`  
`>>> sentence = """At eight o'clock on Thursday morning ... Arthur didn't feel very good."""`  
`>>> tokens = nltk.word_tokenize(sentence)`  
`>>> tokens`  
`['At', 'eight', "o'clock", 'on', 'Thursday', 'morning', 'Arthur', 'did', "n't", 'feel', 'very', 'good', '.']`  
`>>> tagged = nltk.pos_tag(tokens)`  
`>>> tagged[0:6]`  
`[('At', 'IN'), ('eight', 'CD'), ("o'clock", 'JJ'), ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN')]`

Определить именованные объекты:

`>>> entities = nltk.chunk.ne_chunk(tagged)`  
`>>> entities`  
`Tree('S', [('At', 'IN'), ('eight', 'CD'), ("o'clock", 'JJ'), ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN'),`  
`Tree('PERSON', [('Arthur', 'NNP')]), ('did', 'VBD'), ("n't", 'RB'), ('feel', 'VB'), ('very', 'RB'), ('good', 'JJ'), ('.', '.')])`

Показать дерево парсинга:

`>>> from nltk.corpus import treebank`  
`>>> t = treebank.parsed_sents('wsj_0001.mrg')[0]`  
`>>> t.draw()`  

![Parse tree](http://www.nltk.org/_images/tree.gif "Generated parse tree")
***
## [Apache OpenNLP (Java)](https://opennlp.apache.org/docs/1.9.2/manual/opennlp.html)
### Описание
Библиотека Apache OpenNLP - это основанный на машинном обучении инструментарий для обработки текста на естественном языке. Он поддерживает наиболее распространенные задачи NLP, такие как токенизация, сегментация предложений, тегирование части речи, извлечение именованных объектов, разбиение на фрагменты, синтаксический анализ и разрешение кореферентности. Эти задачи обычно требуются для создания более сложных служб обработки текста. OpenNLP также включает в себя максимальную энтропию и машинное обучение на основе персептрона.

Целью проекта OpenNLP будет создание зрелого инструментария для вышеуказанных задач. Дополнительной целью является предоставление большого количества предварительно созданных моделей для различных языков, а также аннотированных текстовых ресурсов, из которых получены эти модели.

### Общая структура библиотеки
Библиотека Apache OpenNLP содержит несколько компонентов, позволяющих создать полноценный конвейер обработки естественного языка. Эти компоненты включают в себя: детектор предложений, токенизатор, искатель имен, классификатор документов, тегер части речи, фрагментатор, синтаксический анализатор, разрешитель кореферентности. Компоненты содержат части, которые позволяют выполнять соответствующую задачу обработки естественного языка, обучать модель и часто также оценивать модель. Каждое из этих средств доступно через программный интерфейс приложения (API). Кроме того, интерфейс командной строки (CLI) предоставляется для удобства экспериментов и обучения.

### Программный интерфейс приложения (API). Общий пример

Компоненты OpenNLP имеют похожие API. Обычно для выполнения задачи необходимо предоставить модель и входные данные.

Модель обычно загружается путем предоставления FileInputStream с моделью конструктора класса модели:
                    
    try (InputStream modelIn = new FileInputStream("lang-model-name.bin")) {
      SomeModel model = new SomeModel(modelIn);
    }
        
После загрузки модели можно создать сам инструмент.
                
    ToolName toolName = new ToolName(model);
        
После того, как инструмент создан, задача обработки может быть выполнена. Форматы ввода и вывода являются специфическими для инструмента, но часто выходные данные представляют собой массив строк, а входные данные представляют собой строку или массив строк.
              
    String output[] = toolName.executeTask("This is a sample text.");
    
***
## [Natasha (Python)](https://github.com/natasha/natasha)
Natasha - библиотека для поиска и извлечения именованных сущностей (Named-entity recognition) из текстов на русском языке. На данный момент разбираются упоминания персон, даты и суммы денег.

### Использование

    from natasha import NamesExtractor

    text = '''
    Простите, еще несколько цитат из приговора. «…Отрицал существование
    Иисуса и пророка Мухаммеда», «наделял Иисуса Христа качествами
    ожившего мертвеца — зомби» [и] «качествами покемонов —
    представителей бестиария японской мифологии, тем самым совершил
    преступление, предусмотренное статьей 148 УК РФ
    '''
    extractor = NamesExtractor()
    matches = extractor(text)
    for match in matches:
        print(match.span, match.fact)

    (69, 75) Name(first='иисус', last=None, middle=None, nick=None)
    (86, 95) Name(first='мухаммед', last=None, middle=None, nick=None)
    (107, 120) Name(first='иисус', last='христос', middle=None, nick=None)

Про атрибуты объекта `match` и другие типы экстракторов написано в [документации](https://natasha.readthedocs.io/ru/latest/).

### [Поиск упоминаний](https://natasha.github.io/demo/)

![Демо поиска упоминаний](https://camo.githubusercontent.com/b18fbe09aae390ae28bde327904776f3e44b09cb/68747470733a2f2f692e696d6775722e636f6d2f3469347372655a2e706e67 "Демо поиска упоминаний")

***
## [Томита-парсер (C++)](https://github.com/yandex/tomita-parser)
Томита-парсер — это инструмент для извлечения структурированных данных (фактов) из текста на естественном языке. Извлечение фактов происходит при помощи контекстно-свободных грамматик и словарей ключевых слов.

### Основные понятия

Томита-парсер позволяет по написанным пользователем шаблонам (КС-грамматикам) выделять из текста разбитые на поля цепочки слов или факты. Например, можно написать шаблоны для выделения адресов. Здесь фактом является адрес, а его полями — название города, название улицы, номер дома и т.д.

Парсер включает в себя три стандартных лингвистических процессора: токенизатор (разбиение на слова), сегментатор (разбиение на предложения) и морфологический анализатор (mystem).

Основные компоненты парсера: газеттир, набор КС-грамматик и множество описаний типов фактов, которые порождаются этими грамматиками в результате процедуры интерпретации.

**Газеттир** — словарь ключевых слов, которые используются в процессе анализа КС-грамматиками. Каждая статья этого словаря задает множество слов и словосочетаний, объединенных общим свойством. Например, все города России. Затем в грамматике можно использовать свойство является городом России. Слова или словосочетания можно задавать явно списком, а можно функционально, указав грамматику, которая описывает нужные цепочки. Например, цепочка ключевых слов адрес описывается соответствующей грамматикой и может быть использована в грамматике для выделения городских происшествий. Подробнее об этом будет в описании механизма каскадов.

**Грамматика** — множество правил на языке КС-грамматик, описывающих синтаксическую структуру выделяемых цепочек. Грамматический парсер запускается всегда на одном предложении. Перед запуском терминалы грамматики отображаются на слова (или словосочетания, об этом будет сказано ниже) предложения. Одному слову может соответствовать много терминальных символов. Таким образом, парсер получает на вход последовательность множеств терминальных символов. Например, в нашей грамматике есть всего два термина Verb и Noun, а входное предложение . Тогда парсер получит на вход такую последовательность: {Noun}, {Verb, Noun}, {Verb, Noun}. На выходе получаются цепочки слов, распознанные этой грамматикой.

**Факты** — таблицы с колонками, которые называются полями фактов. Факты заполняются во время анализа парсером предложения. Как и чем заполнять поля фактов указывается в каждой конкретной грамматике. Это называется интерпретацией. Типы факты описываются на специальном языке в отдельном файле.
