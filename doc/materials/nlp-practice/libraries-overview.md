# [Бібліотеки для синтаксичного аналізу тексту](https://nlpub.mipt.ru/%D0%9E%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0_%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%B0)

## [Natural Language Toolkit (NLTK, Python)](http://www.nltk.org/)
NLTK - це провідна платформа для створення програм на Python для роботи з даними на людській мові. Він надає прості у використанні інтерфейси для більш ніж 50 корпусних та лексичних ресурсів, таких як WordNet, поряд з набором бібліотек обробки тексту для класифікації, токенізації, обробки за мітками, розмітки, аналізу і семантичного мислення, оболонок для промислових бібліотек NLP, і активний форум для обговорень.

Завдяки практичній інструкції, в якій представлені основи програмування, а також темі комп'ютерної лінгвістики і вичерпній документації по API, NLTK підходить для лінгвістів, інженерів, студентів, викладачів, дослідників і користувачів галузі. NLTK доступний для Windows, Mac OS X та Linux. Краще за все те, що NLTK - це безкоштовний проект з відкритим вихідним кодом, керований співтовариством.

NLTK був названий «прекрасним інструментом для навчання і роботи в галузі комп'ютерної лінгвістики з використанням Python» і «дивовижною бібліотекою для гри із природною мовою».

Обробка природної мови за допомогою Python представляє собою практичний вступ у програмування для мовної обробки. Написаний творцями NLTK, він проводить читатача через основи написання програм на Python, роботи з корпусами, категоризації тексту, аналізу мовної структури і багато чого іншого. Онлайн-версія книги була оновлена ​​для Python 3 та NLTK 3.

Деякі прості речі, які ви можете зробити із NLTK:
Токенізувати та помітити текст:


`>>> import nltk`  
`>>> sentence = """At eight o'clock on Thursday morning ... Arthur didn't feel very good."""`  
`>>> tokens = nltk.word_tokenize(sentence)`  
`>>> tokens`  
`['At', 'eight', "o'clock", 'on', 'Thursday', 'morning', 'Arthur', 'did', "n't", 'feel', 'very', 'good', '.']`  
`>>> tagged = nltk.pos_tag(tokens)`  
`>>> tagged[0:6]`  
`[('At', 'IN'), ('eight', 'CD'), ("o'clock", 'JJ'), ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN')]`

Визначити іменовані об'єкти:

`>>> entities = nltk.chunk.ne_chunk(tagged)`  
`>>> entities`  
`Tree('S', [('At', 'IN'), ('eight', 'CD'), ("o'clock", 'JJ'), ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN'),`  
`Tree('PERSON', [('Arthur', 'NNP')]), ('did', 'VBD'), ("n't", 'RB'), ('feel', 'VB'), ('very', 'RB'), ('good', 'JJ'), ('.', '.')])`

Показати дерево парсингу:

`>>> from nltk.corpus import treebank`  
`>>> t = treebank.parsed_sents('wsj_0001.mrg')[0]`  
`>>> t.draw()`  

![Parse tree](http://www.nltk.org/_images/tree.gif "Generated parse tree")
***
## [Apache OpenNLP (Java)](https://opennlp.apache.org/docs/1.9.2/manual/opennlp.html)
### Опис
Бібліотека Apache OpenNLP - це оснований на машинному навчанні інструментарій для обробки тексту на природній мові. Він підтримує найбільш розповсюджені задачі NLP, такі як токенізація, сегментація речень, тегування частини мови, витяг іменованих об'єктів, розбиття на фрагменти, синтаксичний аналіз і розв'язування кореферентності. Ці задачі зазвичай потрібні для створення більш складних служб обробки тексту. OpenNLP також включає в себе максимальну ентропію і машинне навчання на основі персептрону.

Метою проекту OpenNLP буде створення зрілого інструментарію для вищенаведених задач. Додатковою ціллю є надання більшої кількості попередньо створених моделей для різних мов, а також анотованих текстових ресурсів, з яких отримані ці моделі.

### Загальна структура бібліотеки
Бібліотека Apache OpenNLP містить декілька компонентів, що дозволяють створити повноцінний конвеєр обробки природної мови. Ці компоненти включають в себе: детектор речень, токенізатор, шукач імен, класифікатор документів, тегер частини мови, фрагментатор, синтаксичний аналізатор, розв'язувач кореферентності. Компоненти містять частини, що дозволяють виконувати відповідну задачу обробки природної мови, навчати модель і часто також оцінювати модель. Кожний з цих засобів доступний через програмний інтерфейс застосунку (API). Крім цього, інтерфейс командного рядку (CLI) надається для зручності експериментів та навчання.

### Програмний інтерфейс застосунку (API). Загальний приклад

Компоненти OpenNLP мають схожі API. Зазвичай для виконання задачі необхідно надати модель та вхідні дані.

Модель зазвичай завантажується шляхом FileInputStream з моделлю конструктора класу моделі:
                    
    try (InputStream modelIn = new FileInputStream("lang-model-name.bin")) {
      SomeModel model = new SomeModel(modelIn);
    }
        
Після завантаження моделі можна створити сам інструмент.
                
    ToolName toolName = new ToolName(model);
        
Після того, як інструмент створений, задача обробки може бути виконана. Формати введення і виведення є специфічними для інструменту, але часто вихідні дані представляють собою масив рядків, а вхідні дані представляють собою рядок чи масив рядків.
              
    String output[] = toolName.executeTask("This is a sample text.");
    
***
## [Natasha (Python)](https://github.com/natasha/natasha)
Natasha - бібліотека для пошуку і витягу іменованих сутностей (Named-entity recognition) із текстів на російській мові. Наразі розбираються згадки персон, дати і суми грошей.

### Використання

    from natasha import NamesExtractor

    text = '''
    Простите, еще несколько цитат из приговора. «…Отрицал существование
    Иисуса и пророка Мухаммеда», «наделял Иисуса Христа качествами
    ожившего мертвеца — зомби» [и] «качествами покемонов —
    представителей бестиария японской мифологии, тем самым совершил
    преступление, предусмотренное статьей 148 УК РФ
    '''
    extractor = NamesExtractor()
    matches = extractor(text)
    for match in matches:
        print(match.span, match.fact)

    (69, 75) Name(first='иисус', last=None, middle=None, nick=None)
    (86, 95) Name(first='мухаммед', last=None, middle=None, nick=None)
    (107, 120) Name(first='иисус', last='христос', middle=None, nick=None)

Про атрибути об'єкту `match` і інші типи екстракторів написано в [документації](https://natasha.readthedocs.io/ru/latest/).

### [Пошук згадок](https://natasha.github.io/demo/)

![Демо пошуку згадок](https://camo.githubusercontent.com/b18fbe09aae390ae28bde327904776f3e44b09cb/68747470733a2f2f692e696d6775722e636f6d2f3469347372655a2e706e67 "Демо пошуку згадок")

***
## [Томіта-парсер (C++)](https://github.com/yandex/tomita-parser)
Томіта-парсер — це інструмент для витягу структурованих даних (фактів) з тексту на природній мові. Витяг фактів відбувається за допомогою контекстно-вільних граматик і словників ключових слів.

### Основні поняття

Томіта-парсер дозволяє за написаними користувачем шаблонами (КВ-граматикам) виділяти з тексту розбиті на поля ланцюжки слів чи факти. Наприклад, можна написати шаблони для виділення адрес. Тут фактом є адреса, а його полями — назва міста, назва вулиці, номер будинку і т.д.

Парсер включає в себе три стандартні лінгвістичні процесори: токенізатор (розбиття на слова), сегментатор (розбиття на речення) і морфологічний аналізатор (mystem).

Основні компоненти парсеру: газетир, набір КВ-граматик і множина описів типів фактів, які породжуються цими граматиками в результаті процедури інтерпретації.

**Газетир** — словник ключових слів, які використовуються в процесі аналізу КВ-граматиками. Кожна стаття цього словника задає множину слів і словосполучень, об'єднаних загальною властивістю. Наприклад, усі міста Росії. Потім у граматиці можна використовувати властивість є містом Росії. Слова чи словосполучення можна задавати явно списком, а можна функціонально, вказавши граматику, яка описує потрібні ланцюжки. Наприклад, ланцюжок ключових слів адреси описується відповідною граматикою і може бути використаний у граматиці для виділення міських подій. Детальніше про це буде в описі механізму каскадів.

**Граматика** — множина правил мовою КВ-граматик, описуючих синтаксичну структуру ланцюжків, що виділяються. Граматичний парсер запускається завжди на одному реченні. Перед запуском термінали граматики відображаються на слова (чи словосполуки, про це буде сказано нижче) речення. Одному слову може відповідати багато термінальних символів. Таким чином, парсер отримує на вхід послідовність множин термінальних символів.

**Факти** — таблиці з колонками, які називаються полями фактів. Факти заповнюються під час аналізу парсером речення. Як і чим заповнювати поля фактів вказується у кожній конкретній граматиці. Це називається інтерпретацією. Типи фактів описуються спеціальною мовою в окремому файлі.
